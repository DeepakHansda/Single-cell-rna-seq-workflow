Clustering is an unsupervised learning procedure that is used to empirically define groups of cells with similar expression profiles. Its primary purpose is to summarize complex scRNA-seq data into a digestible format for human interpretation. This allows us to describe population heterogeneity in terms of discrete labels that are easily understood, rather than attempting to comprehend the high-dimensional manifold on which the cells truly reside. After annotation based on marker genes, the clusters can be treated as proxies for more abstract biological concepts such as cell types or states.

```r
library(DropletTestFiles)
raw.path <- getTestFile("tenx-2.1.0-pbmc4k/1.0.0/raw.tar.gz")
out.path <- file.path(tempdir(), "pbmc4k")
untar(raw.path, exdir=out.path)

library(DropletUtils)
fname <- file.path(out.path, "raw_gene_bc_matrices/GRCh38")
sce.pbmc <- read10xCounts(fname, col.names=TRUE)

# gene-annotation #
library(scater)
rownames(sce.pbmc) <- uniquifyFeatureNames(
  rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol)
  
library(EnsDb.Hsapiens.v86)
location <- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, 
                   column="SEQNAME", keytype="GENEID")
                   
# Cell detection #
set.seed(1000)
e.out <- emptyDrops(counts(sce.pbmc))
sce.pbmc <- sce.pbmc[,which(e.out$FDR <= 0.001)]

# Quality control #
stats <- perCellQCMetrics(sce.pbmc, subsets=list(Mito=which(location=="MT")))
high.mito <- isOutlier(stats$subsets_Mito_percent, type="higher")
sce.pbmc <- sce.pbmc[,!high.mito]

# Normalization # 
set.seed(10009)
clusters <- quickCluster(sce.pbmc)
sce.pbmc <- computeSumFactors(sce.pbmc, cluster=clusters)
sce.pbmc <- logNormCounts(sce.pbmc)

# Variance modeling #
set.seed(1001)
dec.pbmc <- modelGeneVarByPoisson(sce.pbmc)
top.pbmc <- getTopHVGs(dec.pbmc, prop=0.1)

# Dimensionality reduction # 
set.seed(10010)
sce.pbmc <- denoisePCA(sce.pbmc, subset.row=top.pbmc, technical=dec.pbmc)

set.seed(101000)
sce.pbmc <- runTSNE(sce.pbmc, dimred="PCA")

set.seed(1101000)
sce.pbmc <- runUMAP(sce.pbmc, dimred="PCA")

> sce.pbmc
class: SingleCellExperiment 
dim: 33694 3994 
metadata(1): Samples
assays(2): counts logcounts
rownames(33694): RP11-34P13.3 FAM138A ... AC213203.1 FAM231B
rowData names(2): ID Symbol
colnames(3994): AAACCTGAGAAGGCCT-1 AAACCTGAGACAGACC-1 ...
  TTTGTCAGTTAAGACA-1 TTTGTCATCCCAAGAT-1
colData names(3): Sample Barcode sizeFactor
reducedDimNames(3): PCA TSNE UMAP
mainExpName: NULL
altExpNames(0):
```

Graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets. We first build a graph where each node is a cell that is connected to its nearest neighbors in the high-dimensional space (some thing like in the following figure). Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related. We then apply algorithms to identify “communities” of cells that are more connected to cells in the same community than they are to cells of different communities. Each community represents a cluster that we can use for downstream interpretation.

![image11](https://user-images.githubusercontent.com/85447250/212389295-f3b3ebf0-8b6e-480a-a337-31332b53daca.png)

Fig. Schematic representation of graph based clustering

We will implement graph based clustering by using `clusterCells()` from  **scran**. All calculations are performed using the top PCs to take advantage of data compression and denoising. This function returns a vector containing cluster assignments for each cell in our SingleCellExperiment object

```r
library(scran)
nn.clusters <- clusterCells(sce.pbmc, use.dimred="PCA")
> table(nn.clusters)
nn.clusters
  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
 56 730 598 540 352 126  45 170 854  46 150  75  31 121  84  16 
> 
```
We put the cluster assignments back into our `SingleCellExperiment` object as a factor in the column metadata. This allows us to conveniently visualize the distribution of clusters in a t-SNE plot.

```r
library(scater)
colLabels(sce.pbmc) <- nn.clusters
plotReducedDim(sce.pbmc, "TSNE", colour_by="label")
```

![image12](https://user-images.githubusercontent.com/85447250/212417830-55f5c690-a382-4d8e-ab85-f314771368eb.png)

Fig. t-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster (nn.cluster) from graph-based clustering.

### K-mean clustering 

k-means clustering is a classic vector quantization technique that divides cells into k clusters. Each cell is assigned to the cluster with the closest centroid, which is done by minimizing the within-cluster sum of squares using a random starting configuration for the k centroids. We usually set k to a large value such as the square root of the number of cells to obtain fine-grained clusters. These are not meant to be interpreted directly, but rather, the centroids are used in downstream steps for faster computation. The main advantage of this approach lies in its speed, given the simplicity and ease of implementation of the algorithm.

```r
library(bluster)
set.seed(100)
clust.kmeans <- clusterCells(sce.pbmc, use.dimred="PCA", 
                             BLUSPARAM=KmeansParam(centers=10))
# table(clust.kmeans)

colLabels(sce.pbmc) <- clust.kmeans
plotReducedDim(sce.pbmc, "TSNE", colour_by="label")
```
![image13](https://user-images.githubusercontent.com/85447250/212501862-57612560-6a3b-4ff6-8a55-4e137409366c.png)

Fig. t-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from k-means clustering. 


### Two-step procedures (First use k-means then graph clustering to achieve cell groups)

Strategy of using k-means as a stand alone method of clustering suffers from several shortcomings that reduce its appeal for obtaining interpretable clusters:

+ It implicitly favors spherical clusters of equal radius. This can lead to unintuitive partitionings on real datasets that contain groupings with irregular sizes and shapes.
+ The number of clusters k must be specified beforehand and represents a hard cap on the resolution of the clustering.. For example, setting k to be below the number of cell types will always lead to co-clustering of two cell types, regardless of how well separated they are. In contrast, other methods like graph-based clustering will respect strong separation even if the relevant resolution parameter is set to a low value.
+ It is dependent on the randomly chosen initial coordinates. This requires multiple runs to verify that the clustering is stable.

However we can adress these concerns by using k-means as a prelude to more sophisticated and interpretable - but computationally expensive - clustering algorithms. The `clusterCells()` function supports a “two-step” mode where k-means is initially used to obtain representative centroids that are subjected to graph-based clustering. Each cell is then placed in the same graph-based cluster that its k-means centroid was assigned to:

```r
set.seed(0101010)
kgraph.clusters <- clusterCells(sce.pbmc, use.dimred="PCA",
                                BLUSPARAM=TwoStepParam(
                                  first=KmeansParam(centers=1000),
                                  second=NNGraphParam(k=5)
                                )
)
> table(kgraph.clusters)
kgraph.clusters
  1   2   3   4   5   6   7   8   9  10  11  12 
490 187 540 732 166 565 905 119 146  45  16  83 
```

![image14](https://user-images.githubusercontent.com/85447250/212503651-37ae3160-79af-449a-8710-34a6c3326280.png)

Fig. t-SNE plot of the PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from combined k-means/graph-based clustering.

The obvious benefit of this approach over direct graph-based clustering is the speed improvement. We avoid the need to identifying nearest neighbors for each cell and the construction of a large intermediate graph, while benefiting from the relative interpretability of graph-based clusters compared to those from k-means. This approach also mitigates the “inflation” effect discussed previously (The main drawback of graph-based methods is that, after graph construction, no information is retained about relationships beyond the neighboring cells. This has some practical consequences in datasets that exhibit differences in cell density, as more steps through the graph are required to move the same distance through a region of higher cell density. From the perspective of community detection algorithms, this effect “inflates” the high-density regions such that any internal substructure or noise is more likely to cause formation of subclusters. The resolution of clustering thus becomes dependent on the density of cells, which can occasionally be misleading if it overstates the heterogeneity in the data.). Each centroid serves as a representative of a region of space that is roughly similar in volume, ameliorating differences in cell density that can cause (potentially undesirable) differences in resolution.
















